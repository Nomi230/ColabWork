{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nomi230/ColabWork/blob/main/decode_without_lm_working_updated_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faddd687-07fe-40b8-96e5-0c4ccea0b275",
      "metadata": {
        "id": "faddd687-07fe-40b8-96e5-0c4ccea0b275"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TRANSFORMERS_CACHE'] =\"/scratch/project_2007260/HF_cache\"\n",
        "os.environ['HF_DATASETS_CACHE'] =\"/scratch/project_2007260/HF_cache\"\n",
        "os.environ['PYTHONPATH']=\"/scratch/project_2007260/miniconda/envs/pytest\"\n",
        "os.environ['CONDA_PREFIX']=\"/scratch/project_2007260/miniconda/envs/pytest\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93c15501-8b5c-4fd7-a53a-385d61f98fd0",
      "metadata": {
        "id": "93c15501-8b5c-4fd7-a53a-385d61f98fd0",
        "outputId": "9311b9f0-7107-44fb-c111-6d3a1c608c81"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/abbasino/.local/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n",
            "2024-07-27 18:59:46.808171: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-27 18:59:47.205765: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-07-27 18:59:50.755799: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-27 19:00:21.505766: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import IPython.display as ipd\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, WhisperProcessor, WhisperForConditionalGeneration, AutoProcessor,HubertModel, HubertForSequenceClassification, HubertForCTC\n",
        "from transformers import Wav2Vec2ProcessorWithLM\n",
        "import torch\n",
        "import pyctcdecode\n",
        "import soundfile as sf\n",
        "import torch\n",
        "from jiwer import wer\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from pyctcdecode import build_ctcdecoder\n",
        "import soundfile as sf\n",
        "import pandas\n",
        "import librosa\n",
        "from jiwer import wer, process_words, visualize_alignment, Compose, ToLowerCase, RemoveWhiteSpace, RemoveMultipleSpaces, ReduceToListOfListOfWords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9ec2b2b-8d87-4e4d-9a15-5c62dcc871c6",
      "metadata": {
        "id": "d9ec2b2b-8d87-4e4d-9a15-5c62dcc871c6"
      },
      "outputs": [],
      "source": [
        "pytorch_device = torch.device(\"cuda:0\")\n",
        "torch.set_num_threads(1)\n",
        "sample_rate=16000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d78b015",
      "metadata": {
        "id": "8d78b015",
        "outputId": "681c9b95-696d-4b4d-e193-d1debd3ab32f"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (3835713575.py, line 17)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 17\u001b[0;36m\u001b[0m\n\u001b[0;31m    nan_count = df[].isna().sum()\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base.en\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base.en\").to(\"cuda:0\")\n",
        "\n",
        "wav_path=\"/scratch/project_2007260/cv_aged_voice/FinalDataset/ASR/wavp_eighties_combined_test\"\n",
        "text_path=\"/scratch/project_2007260/cv_aged_voice/FinalDataset/ASR/tid_eighties_combined_test\"\n",
        "\n",
        "wav_l = [temp.strip() for temp in open(wav_path).readlines()]\n",
        "text_l = [temp.strip() for temp in open(text_path).readlines()]\n",
        "df = pandas.DataFrame(data={\"file\": wav_l, \"audio\": wav_l, \"text\": text_l})\n",
        "\n",
        "df.to_csv(\"test.csv\", sep=\",\",index=False)\n",
        "\n",
        "dataFiles = {'test': \"test.csv\"}\n",
        "ld = load_dataset('csv', data_files=dataFiles)\n",
        "\n",
        "eval_data = ld[\"test\"]\n",
        "sample_rate=16000\n",
        "device=\"cuda:0\"\n",
        "\n",
        "ref = []\n",
        "res = []\n",
        "punc = '''!()-[]{};.:'\"\\,<>./?@#$%^&*_~'''\n",
        "\n",
        "for i in range(len(eval_data)):\n",
        "    audio_sample = eval_data[i]\n",
        "    if os.path.exists(audio_sample[\"file\"]):\n",
        "     audio, _ = librosa.load(audio_sample[\"file\"], sr=16000)\n",
        "     test_str=audio_sample[\"text\"].lower()\n",
        "     stk=\"\"\n",
        "     for ele in test_str:\n",
        "        if ele not in punc:\n",
        "            stk+=ele\n",
        "     ref.append(stk)\n",
        "\n",
        "     input_features = processor(audio, sampling_rate=sample_rate, return_tensors=\"pt\").input_features.to(device)\n",
        "     predicted_ids = model.generate(input_features)\n",
        "     transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "\n",
        "     test_str=transcription[0].lower()\n",
        "     stk=\"\"\n",
        "     for ele in test_str:\n",
        "        if ele not in punc:\n",
        "            stk+=ele\n",
        "     res.append(stk)\n",
        "     if i%1000 == 0:\n",
        "        print(ref[i],res[i])\n",
        "\n",
        "\n",
        "transformation = Compose([\n",
        "    ToLowerCase(),\n",
        "    RemoveWhiteSpace(replace_by_space=True),\n",
        "    RemoveMultipleSpaces(),\n",
        "    ReduceToListOfListOfWords(word_delimiter=\" \")\n",
        "])\n",
        "\n",
        "out = process_words(ref,res)\n",
        "\n",
        "print(\"WER Whisper:\", wer(ref,res,truth_transform=transformation,hypothesis_transform=transformation)*100, \"Substitutions:\", out.substitutions, \" Deletions:\", out.deletions, \" Insertions:\",out.insertions)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77fd495c",
      "metadata": {
        "id": "77fd495c",
        "outputId": "66fc1359-6cb1-4665-9654-1696b3a8e3d6",
        "colab": {
          "referenced_widgets": [
            "7bde9411b5714f0e9cd4f99021ca98ae"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bde9411b5714f0e9cd4f99021ca98ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "layne took many trips to africa after studying african history lan took many trips to africa after studying african history\n",
            "WER Wav2Vec Model: 10.405257393209201 Substitutions: 141  Deletions: 12  Insertions: 37\n"
          ]
        }
      ],
      "source": [
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\").to(\"cuda:0\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
        "\n",
        "wav_path=\"/scratch/project_2007260/cv_aged_voice/FinalDataset/ASR/wavp_eighties_combined_test\"\n",
        "text_path=\"/scratch/project_2007260/cv_aged_voice/FinalDataset/ASR/tid_eighties_combined_test\"\n",
        "\n",
        "wav_l = [temp.strip() for temp in open(wav_path).readlines()]\n",
        "text_l = [temp.strip() for temp in open(text_path).readlines()]\n",
        "df = pandas.DataFrame(data={\"file\": wav_l, \"audio\": wav_l, \"text\": text_l})\n",
        "\n",
        "df.to_csv(\"test.csv\", sep=\",\",index=False)\n",
        "\n",
        "dataFiles = {'test': \"test.csv\"}\n",
        "ld = load_dataset('csv', data_files=dataFiles)\n",
        "\n",
        "eval_data = ld[\"test\"]\n",
        "\n",
        "\n",
        "sample_rate=16000\n",
        "device=\"cuda:0\"\n",
        "\n",
        "ref = []\n",
        "res = []\n",
        "punc = '''!()-[]{};.:'\"\\,<>./?@#$%^&*_~'''\n",
        "\n",
        "for i in range(len(eval_data)):\n",
        "   audio_sample = eval_data[i]\n",
        "   if os.path.exists(audio_sample[\"file\"]):\n",
        "     audio, _ = librosa.load(audio_sample[\"file\"], sr=16000)\n",
        "     test_str=audio_sample[\"text\"].lower()\n",
        "     stk=\"\"\n",
        "     for ele in test_str:\n",
        "        if ele not in punc:\n",
        "            stk+=ele\n",
        "     ref.append(stk)\n",
        "\n",
        "     input_values = processor(audio, return_tensors=\"pt\",\n",
        "                                     sampling_rate=sample_rate).input_values.to(device)\n",
        "     with torch.no_grad():\n",
        "        logits = model(input_values).logits\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "        log_processed = logits.cpu().numpy()[0]\n",
        "     transcription = processor.batch_decode(predicted_ids)\n",
        "\n",
        "     test_str=transcription[0].lower()\n",
        "     stk=\"\"\n",
        "     for ele in test_str:\n",
        "        if ele not in punc:\n",
        "            stk+=ele\n",
        "     res.append(stk)\n",
        "     if i%1000 == 0:\n",
        "        print(ref[i],res[i])\n",
        "\n",
        "transformation = Compose([\n",
        "    ToLowerCase(),\n",
        "    RemoveWhiteSpace(replace_by_space=True),\n",
        "    RemoveMultipleSpaces(),\n",
        "    ReduceToListOfListOfWords(word_delimiter=\" \")\n",
        "])\n",
        "\n",
        "out = process_words(ref,res)\n",
        "\n",
        "print(\"WER Wav2Vec Model:\", wer(ref,res,truth_transform=transformation,hypothesis_transform=transformation)*100, \"Substitutions:\", out.substitutions, \" Deletions:\", out.deletions, \" Insertions:\",out.insertions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0fdfb3f",
      "metadata": {
        "id": "c0fdfb3f",
        "outputId": "79736dd1-00f7-4e6b-8f15-24736f1ad391",
        "colab": {
          "referenced_widgets": [
            "bfac7d28547b451bbcbbf1f741622188"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfac7d28547b451bbcbbf1f741622188",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "layne took many trips to africa after studying african history lan took many trips africa after studying african history\n",
            "WER from  Model MMS: 10.952902519167578 Substitutions: 156  Deletions: 14  Insertions: 30\n"
          ]
        }
      ],
      "source": [
        "from transformers import Wav2Vec2ForCTC, AutoProcessor\n",
        "\n",
        "model_id = \"facebook/mms-1b-all\"\n",
        "target_lang = \"eng\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id, target_lang=target_lang)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_id, target_lang=target_lang, ignore_mismatched_sizes=True).to(device)\n",
        "\n",
        "wav_path=\"/scratch/project_2007260/cv_aged_voice/FinalDataset/ASR/wavp_eighties_combined_test\"\n",
        "text_path=\"/scratch/project_2007260/cv_aged_voice/FinalDataset/ASR/tid_eighties_combined_test\"\n",
        "\n",
        "wav_l = [temp.strip() for temp in open(wav_path).readlines()]\n",
        "text_l = [temp.strip() for temp in open(text_path).readlines()]\n",
        "df = pandas.DataFrame(data={\"file\": wav_l, \"audio\": wav_l, \"text\": text_l})\n",
        "\n",
        "df.to_csv(\"test.csv\", sep=\",\",index=False)\n",
        "\n",
        "dataFiles = {'test': \"test.csv\"}\n",
        "ld = load_dataset('csv', data_files=dataFiles)\n",
        "\n",
        "eval_data = ld[\"test\"]\n",
        "\n",
        "\n",
        "sample_rate=16000\n",
        "device=\"cuda:0\"\n",
        "\n",
        "ref = []\n",
        "res = []\n",
        "punc = '''!()-[]{};.:'\"\\,<>./?@#$%^&*_~'''\n",
        "\n",
        "for i in range(len(eval_data)):\n",
        "   audio_sample = eval_data[i]\n",
        "   if os.path.exists(audio_sample[\"file\"]):\n",
        "     audio, sampling_rate= librosa.load(audio_sample[\"file\"], sr=16000)\n",
        "     test_str=audio_sample[\"text\"].lower()\n",
        "     stk=\"\"\n",
        "     for ele in test_str:\n",
        "        if ele not in punc:\n",
        "            stk+=ele\n",
        "     ref.append(stk)\n",
        "\n",
        "     inputs = processor(audio, sampling_rate=sampling_rate, return_tensors=\"pt\").to(device)\n",
        "     with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "     predicted_ids = torch.argmax(logits, dim=-1)\n",
        "     transcription = processor.batch_decode(predicted_ids)\n",
        "\n",
        "     test_str=transcription[0].lower()\n",
        "     stk=\"\"\n",
        "     for ele in test_str:\n",
        "        if ele not in punc:\n",
        "            stk+=ele\n",
        "     res.append(stk)\n",
        "     if i%1000 == 0:\n",
        "        print(ref[i],res[i])\n",
        "\n",
        "transformation = Compose([\n",
        "    ToLowerCase(),\n",
        "    RemoveWhiteSpace(replace_by_space=True),\n",
        "    RemoveMultipleSpaces(),\n",
        "    ReduceToListOfListOfWords(word_delimiter=\" \")\n",
        "])\n",
        "\n",
        "out = process_words(ref,res)\n",
        "\n",
        "print(\"WER from  Model MMS:\", wer(ref,res,truth_transform=transformation,hypothesis_transform=transformation)*100, \"Substitutions:\", out.substitutions, \" Deletions:\", out.deletions, \" Insertions:\",out.insertions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95d8ed73",
      "metadata": {
        "id": "95d8ed73",
        "outputId": "273b5b34-4a8e-496f-9168-65374fb92636",
        "colab": {
          "referenced_widgets": [
            "6c9fae781dc844d28291cb085a578c4e"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c9fae781dc844d28291cb085a578c4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "layne took many trips to africa after studying african history lan took many trips to africa after studying african history\n",
            "WER from Hubert Model: 11.610076670317634 Substitutions: 155  Deletions: 16  Insertions: 41\n"
          ]
        }
      ],
      "source": [
        "processor = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
        "model = HubertForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\").to(\"cuda:0\")\n",
        "wav_path=\"/scratch/project_2007260/cv_aged_voice/FinalDataset/ASR/wavp_eighties_combined_test\"\n",
        "text_path=\"/scratch/project_2007260/cv_aged_voice/FinalDataset/ASR/tid_eighties_combined_test\"\n",
        "\n",
        "wav_l = [temp.strip() for temp in open(wav_path).readlines()]\n",
        "text_l = [temp.strip() for temp in open(text_path).readlines()]\n",
        "df = pandas.DataFrame(data={\"file\": wav_l, \"audio\": wav_l, \"text\": text_l})\n",
        "\n",
        "df.to_csv(\"test.csv\", sep=\",\",index=False)\n",
        "\n",
        "dataFiles = {'test': \"test.csv\"}\n",
        "ld = load_dataset('csv', data_files=dataFiles)\n",
        "\n",
        "eval_data = ld[\"test\"]\n",
        "\n",
        "\n",
        "sample_rate=16000\n",
        "device=\"cuda:0\"\n",
        "\n",
        "ref = []\n",
        "res = []\n",
        "punc = '''!()-[]{};.:'\"\\,<>./?@#$%^&*_~'''\n",
        "\n",
        "for i in range(len(eval_data)):\n",
        "    audio_sample = eval_data[i]\n",
        "    if os.path.exists(audio_sample[\"file\"]):\n",
        "        audio, sampling_rate= librosa.load(audio_sample[\"file\"], sr=16000)\n",
        "        test_str=audio_sample[\"text\"].lower()\n",
        "        stk=\"\"\n",
        "        for ele in test_str:\n",
        "            if ele not in punc:\n",
        "                stk+=ele\n",
        "        ref.append(stk)\n",
        "\n",
        "        inputs = processor(audio, sampling_rate=sampling_rate, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = model(**inputs).logits\n",
        "            predicted_ids = torch.argmax(logits, dim=-1)\n",
        "        transcription = processor.batch_decode(predicted_ids)\n",
        "\n",
        "        test_str=transcription[0].lower()\n",
        "        stk=\"\"\n",
        "        for ele in test_str:\n",
        "            if ele not in punc:\n",
        "                stk+=ele\n",
        "        res.append(stk)\n",
        "        if i%1000 == 0:\n",
        "            print(ref[i],res[i])\n",
        "\n",
        "transformation = Compose([\n",
        "    ToLowerCase(),\n",
        "    RemoveWhiteSpace(replace_by_space=True),\n",
        "    RemoveMultipleSpaces(),\n",
        "    ReduceToListOfListOfWords(word_delimiter=\" \")\n",
        "])\n",
        "\n",
        "out = process_words(ref,res)\n",
        "\n",
        "print(\"WER from Hubert Model:\", wer(ref,res,truth_transform=transformation,hypothesis_transform=transformation)*100, \"Substitutions:\", out.substitutions, \" Deletions:\", out.deletions, \" Insertions:\",out.insertions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3b39c23",
      "metadata": {
        "id": "f3b39c23"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}